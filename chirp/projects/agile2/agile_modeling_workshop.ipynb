{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Build Your Own Call Recogniser\n",
    "\n",
    "_Integrating Passive Acoustic Monitoring with AI for Scalable Biodiversity Tracking_"
   ],
   "metadata": {
    "id": "lp0RzLu9KerR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to the agile modelling Python notebook."
   ],
   "metadata": {
    "id": "LId8il9BKiTU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What is a Python notebook?\n",
    "\n",
    "A Python notebook allows you to run Python code in a Python environment. If you are running this notebook in Google Colab, the Python notebook is running in a virtual machine in the cloud."
   ],
   "metadata": {
    "id": "yobMfr-RKpzX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Do I need to be familiar with Python?\n",
    "\n",
    "No, you do not need to be familiar with Python to work through the notebook. You will interact with the notebook via UI elements such as text boxes, dropdown menus, and buttons.\n",
    "\n",
    "In fact, most of the Python code in the notebook is hidden by default to allow you to focus on the agile modelling workflow itself. If you are curious to look behind the curtain, you can click a code cell's \"Show code\" button like so:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://storage.googleapis.com/chirp-public-bucket/esa-2024/reveal_code.png\" width=\"500\"/>\n",
    "</div>"
   ],
   "metadata": {
    "id": "DkKLhINiKsq8"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbqUAfBVnQFO"
   },
   "source": [
    "### Notebook overview\n",
    "\n",
    "In this notebook, we will use a process called \"[agile modeling](https://arxiv.org/abs/2302.12948)\" to build and incrementally improve a classifier for acoustic analysis, starting from a single classified example. The process uses embeddings provided by the [Perch model](https://www.kaggle.com/models/google/bird-vocalization-classifier). These are the steps we will take:\n",
    "\n",
    "1. Setup\n",
    "2. Configure the Perch agile modelling modules\n",
    "3. Create a database of embeddings\n",
    "4. Search for recordings similar to the annotator-provided example\n",
    "5. Build a machine learning classifier model from the search results\n",
    "6. Search your recordings based on the results of the classifier\n",
    "7. Improve your classifier further using these search results\n",
    "\n",
    "The agile modelling process is described in more detail in [these slides](https://docs.google.com/presentation/d/e/2PACX-1vTfvoBvCi_V72s0RiIcmFNdnZDcPDCDl-omBbODJ3sz3_IxD5kd1zJjd-J8AR7PE_DgxO-FWDjyP7Mb/pub?start=false&loop=false&delayms=3000&slide=id.g2d63d0c2ccf_0_3915) and in the following diagram:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://storage.googleapis.com/chirp-public-bucket/esa-2024/agile_modelling_workflow.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setup\n",
    "\n",
    "You are running this notebook in a Python environment. We need to add the Perch package to this environment. You only need to do this once, however if you are running this notebook in the cloud on Google Colab, your session is only ephemaral. You will need to rerun his this cell after disconnecting.\n",
    "\n",
    "> **NOTE: The session needs to be restarted after this step.**"
   ],
   "metadata": {
    "id": "VNdr15dmLvk5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Install the `perch` package and import requirements\n",
    "#@markdown <font color='green'>\u2190 Run this cell to install the Perch package and\n",
    "#@markdown import requirements.</font>\n",
    "#@markdown\n",
    "#@markdown After running this cell for the first time, you need to restart your\n",
    "#@markdown session in order for the changes made by installing the Perch package\n",
    "#@markdown take effect.\n",
    "#@markdown\n",
    "#@markdown You should be automatically prompted for a session restart, but if\n",
    "#@markdown you are not please manually restart the session like so:\n",
    "#@markdown\n",
    "#@markdown <div>\n",
    "#@markdown <img src=\"https://storage.googleapis.com/chirp-public-bucket/esa-2024/restart_session.png\" width=\"300\"/>\n",
    "#@markdown </div>\n",
    "#@markdown\n",
    "#@markdown > **NOTE: after restarting the session, you need to run this cell\n",
    "#@markdown > again.**\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "from IPython import display as ipython_display\n",
    "\n",
    "display(ipython_display.Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 128})'''))\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "try:\n",
    "  from chirp.projects.agile2 import agile_modeling_state\n",
    "\n",
    "  agile2_config = agile_modeling_state.agile2_config\n",
    "  agile2_state = agile_modeling_state.agile2_state\n",
    "  download_embeddings = agile_modeling_state.download_embeddings\n",
    "  Helpers = agile_modeling_state.Helpers\n",
    "\n",
    "  import dotenv\n",
    "\n",
    "  find_dotenv = dotenv.find_dotenv\n",
    "  load_dotenv = dotenv.load_dotenv\n",
    "\n",
    "  if not __import__('numpy').__version__.startswith('1.24'):\n",
    "    print(\n",
    "        'Make sure you have restarted the session after installing Perch,'\n",
    "        ' following the instructions above.'\n",
    "    )\n",
    "except ImportError:\n",
    "  !pip install git+https://github.com/QutEcoacoustics/perch.git@ce15ee0cf9f3bc4bb44e91b0f4e2dc377ff5902b python-dotenv"
   ],
   "metadata": {
    "id": "tiPc5ofSLzj3",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Link to Google Drive\n",
    "#@markdown <font color='green'>\u2190 Run this cell to link to Google Drive.</font>\n",
    "#@markdown\n",
    "#@markdown We will need somewhere to read and write files. This colab environment\n",
    "#@markdown where the notebook is running does not persist between sessions, so\n",
    "#@markdown we will link to google drive for access to persistent storage.\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "except:\n",
    "    print(\"colab not available\")"
   ],
   "metadata": {
    "cellView": "form",
    "id": "_2yCNFHBNOwF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnsSbQa5nQFP"
   },
   "source": [
    "## 2. Configure the Perch agile modelling modules\n",
    "\n",
    "Here we set some configuration for names and local filepaths and initialize our agile modeling workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbQDsw0hnQFP",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Configure the agile modelling workflow\n",
    "#@markdown <font color='green'>\u2190 Run this cell to configure the agile modelling\n",
    "#@markdown workflow.</font>\n",
    "annotator_id = \"phil\"  # @param {type:'string'}\n",
    "#@markdown The purpose of the `annotator_id` variable above is to keep track of\n",
    "#@markdown who provided each annotation when collaborating across multiple\n",
    "#@markdown annotators.\n",
    "\n",
    "# This is the location on google drive that this tutorial will use to save data.\n",
    "# If you followed the above instructions for creating a shortcut to the Drive\n",
    "# folder, you should be able to navigate to this directory in the left hand\n",
    "# \"Files\" menu in this Colab (indicated by the Folder icon on the far left\n",
    "# menu).\n",
    "working_folder = pathlib.Path('/content/drive/My Drive/esa2024_data')\n",
    "\n",
    "config = agile2_config(\n",
    "    db_path=working_folder / 'db' / 'db.sqlite',\n",
    "    annotator_id=annotator_id,\n",
    "    # For this tutorial we arbitrarily choose to name our search dataset\n",
    "    # \"search_set\", however if you manage multiple search datasets you will have\n",
    "    # to assign each a unique name via this configuration.\n",
    "    search_dataset_name=\"search_set\",\n",
    "    embeddings_folder=pathlib.Path('/content/embeddings'),\n",
    "    labeled_examples_folder=working_folder / 'labeled_examples',\n",
    "    models_folder=working_folder / 'models',\n",
    "    predictions_folder=working_folder / 'predictions',\n",
    ")\n",
    "\n",
    "config.labeled_examples_folder.mkdir(exist_ok=True, parents=True)\n",
    "config.embeddings_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "config.baw_config['domain'] = 'api.ecosounds.org'\n",
    "\n",
    "agile = agile2_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSI2gNN8HV-c",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Configure access to your Ecosounds project\n",
    "#@markdown <font color='green'>\u2190 Run this cell after reading the instructions\n",
    "#@markdown below to configure access to your\n",
    "#@markdown Ecosounds project.</font>\n",
    "#@markdown\n",
    "#@markdown > **NOTE: If you are _not_ using your own data for the tutorial, you can skip this step.**\n",
    "#@markdown\n",
    "#@markdown We will be loading audio from [Ecosounds](https://www.ecosounds.org),\n",
    "#@markdown an online repository of ecoacoustic recordings. If working with a\n",
    "#@markdown private Ecosounds project, we need to provide the Ecosounds *auth\n",
    "#@markdown token* associated with your ecosounds account.\n",
    "#@markdown\n",
    "#@markdown To find your ecosounds token, go to https://www.ecosounds.org/my_account\n",
    "#@markdown and in the bottom left, click on the button to copy the token like so:\n",
    "#@markdown\n",
    "#@markdown <div><img src=\"https://storage.googleapis.com/chirp-public-bucket/esa-2024/ecosounds_token.png\" width=\"800\"/></div>\n",
    "#@markdown\n",
    "#@markdown Because this is a secret, we should avoid saving it in plain text in\n",
    "#@markdown the notebook, so we will set it up in an environment variable.\n",
    "#@markdown Depending on where you are running this notebook, do one of the\n",
    "#@markdown following (if you are not sure, just run this cell and it will tell you).\n",
    "#@markdown\n",
    "#@markdown > **Colab**\n",
    "#@markdown\n",
    "#@markdown <div><img src=\"https://storage.googleapis.com/chirp-public-bucket/esa-2024/colab_secrets.png\" width=\"500\"/></div>\n",
    "#@markdown\n",
    "#@markdown 1. On the right, click on the key icon to open the \"secrets\" tab.\n",
    "#@markdown 2. Click \"Add new secret\"\n",
    "#@markdown 3. Under \"Name\" put the text `BAW_AUTH_TOKEN` (without quotes)\n",
    "#@markdown 4. Under \"Value\" paste the token you copied from Ecosounds\n",
    "#@markdown\n",
    "#@markdown > **Jupyter, running locally**\n",
    "#@markdown\n",
    "#@markdown 1. In the working directory create a file named `.env`\n",
    "#@markdown    - The working directory is probably the directory where you launched the notebook. If you are not sure, run the cell below and it will tell you.\n",
    "#@markdown 2. In this .env file, put the line `BAW_AUTH_TOKEN=abc123xyz` (replace `abc123xyz` with the token you copied from Ecosounds)\n",
    "auth_token = None\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    auth_token = userdata.get('BAW_AUTH_TOKEN')\n",
    "    print(\"Got auth token from colab secrets\")\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    env_file = find_dotenv()\n",
    "    if not env_file:\n",
    "        print(f\"No .env file found in the working directory {os.getcwd()}. \\nFollow the local Jupyter instructions above to create one.\")\n",
    "    else:\n",
    "        load_dotenv(override=True)\n",
    "        auth_token = os.getenv('BAW_AUTH_TOKEN')\n",
    "        if auth_token:\n",
    "            print(f\"Got auth token from .env file {env_file}\")\n",
    "        else:\n",
    "            print(\"BAW_AUTH_TOKEN env variable not found in your .env file. Follow the local Jupyter instructions above to set it\")\n",
    "\n",
    "except userdata.SecretNotFoundError:\n",
    "        print(\"No BAW_AUTH_TOKEN secret found, please follow the colab instructions above to set it\")\n",
    "\n",
    "if auth_token:\n",
    "    if config.baw_config.get('auth_token'):\n",
    "        print(\"Overwriting config auth token with new value\")\n",
    "    config.baw_config['auth_token'] = auth_token\n",
    "elif config.baw_config.get('auth_token'):\n",
    "    print(\"Auth token not loaded, but already in config\")\n",
    "else:\n",
    "    print(\"No auth token set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxukSWBCnQFQ"
   },
   "source": [
    "## 3. Create a database of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What is an embedding?\n",
    "\n",
    "Think of an embedding as a summary of the audio waveform in the form of an array of numbers. In the context of deep learning (which Perch relies on), that array of numbers is obtained by passing a _spectrogram_ representation of the audio through a deep neural network.\n",
    "\n",
    "The exact details can be safely ignored for the purpose of this tutorial, but the neural network was constructed and trained in such a way that the relationship between the embeddings it outputs tend to be consistent with the semantic relationships between the audio waveforms (e.g., passing two different vocalizations for the same animal species through the neural network will produce similar embeddings).\n",
    "\n",
    "We will exploit this property in our agile modelling workflow.\n",
    "\n",
    "The first step in the workflow is therefore to compute an embedding with Perch for every possible 5-seconds audio clip extracted from our audio recordings. In the interest of time, those embeddings have already been computed and the work needed for this step is to download the embeddings locally and format them appropriately.\n",
    "\n",
    "If you have uploaded audio for this workshop, you will have been given a name to use for that search set.\n",
    "\n",
    "Otherwise, you can use one of the following public search sets:\n",
    "\n",
    "1. `yellow_bellied_glider`\n",
    "2. `gympie`\n",
    "3. `gympie_small` (a smaller dataset that takes less time to prepreocess and search through)\n",
    "4. `forty_spotted_pardalote`\n"
   ],
   "metadata": {
    "id": "XFRHcrs9cWgl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWYxjkG2nQFQ",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Download audio embeddings locally\n",
    "#@markdown <font color='green'>\u2190 Run this cell after making a choice below to\n",
    "#@markdown download audio embeddings.</font>\n",
    "#@markdown\n",
    "#@markdown Choose a publicly-available option in the dropdown or type in the\n",
    "#@markdown name of the dataset you provided.\n",
    "dataset_name = \"yellow_bellied_glider\" # @param [\"yellow_bellied_glider\",\"gympie\",\"gympie_small\",\"forty_spotted_pardalote\"] {\"allow-input\":true}\n",
    "\n",
    "download_from_gcp = dataset_name in [\n",
    "    \"yellow_bellied_glider\",\n",
    "    \"gympie\",\n",
    "    \"gympie_small\",\n",
    "    \"forty_spotted_pardalote\",\n",
    "]\n",
    "\n",
    "download_embeddings(\n",
    "    dataset_name,\n",
    "    config.embeddings_folder,\n",
    "    download_from_gcp=download_from_gcp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6yH31NqnQFQ",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Create a database of embeddings\n",
    "#@markdown <font color='green'>\u2190 Run this cell to create the embeddings database.</font>\n",
    "#@markdown\n",
    "#@markdown The database links labels to embeddings so we can train our classifier.\n",
    "\n",
    "agile.create_database(config.embeddings_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njS1zcv-nQFQ",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Initialize the agile modelling workflow\n",
    "#@markdown <font color='green'>\u2190 Run this cell to initialize the workflow.\n",
    "\n",
    "agile.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVdLJJd9gnjo"
   },
   "source": [
    "## 4. Search for recordings similar to the annotator-provided example\n",
    "\n",
    "Here, we take a single example and find the examples in our search set which most closely match that example. This is a way to get started with a labelled training set.\n",
    "\n",
    "You have three options to provide examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gK6hsJDInQFQ",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title A. Copying your own files in Google Drive\n",
    "#@markdown <font color='orange'>\u2190 [OPTIONAL] Run this cell to list audio files\n",
    "#@markdown in your mounted Google Drive folder.</font>\n",
    "#@markdown\n",
    "#@markdown If you have short examples of your target call, copy them into the\n",
    "#@markdown `config.labeled_examples_folder` directory and then run this cell to\n",
    "#@markdown check that they are accessible.\n",
    "#@markdown\n",
    "#@markdown If you are unsure where that is, running this cell will display the\n",
    "#@markdown path to `config.labeled_examples_folder`.\n",
    "\n",
    "audio_files = Helpers.list_audio_files(config.labeled_examples_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### B. Use the examples provided in our shared Google Drive folder\n",
    "\n",
    "We also have some labelled examples in a shared Google Drive folder. If you want to use those:\n",
    "\n",
    "- Navigate to the shared data [Google Drive folder](https://drive.google.com/drive/folders/1SQi-VunCpnqrPcQpaDrt2-VzVJigZvU9).\n",
    "- Click the dropdown menu labeled `labeled_examples`.\n",
    "- Select `Organize` -> `Add shortcut`.\n",
    "- Choose somewhere in your Google Drive to add the shortcut. You will use the path to this location later.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://storage.googleapis.com/chirp-public-bucket/esa-2024/shared_labeled_examples.png\" width=\"800\"/>\n",
    "</div>"
   ],
   "metadata": {
    "id": "MmBOY5HrFAyO"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtE3uy-TnQFQ"
   },
   "source": [
    "### C. Provide a URL or Xeno Canto ID\n",
    "\n",
    "You can also directly provide a URL pointing to the example or a Xeno-Canto ID in the form `xc123456`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ig3L5dsy3mr",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Load the query audio { vertical-output: true }\n",
    "#@markdown <font color='green'>\u2190 Run this cell after reading the instructions\n",
    "#@markdown below to load the query audio.</font>\n",
    "query = '1' # @param {type:'string'}\n",
    "#@markdown The `query` above can be\n",
    "#@markdown 1. one of the integer indices listed after running the cell for\n",
    "#@markdown    option A above; or\n",
    "#@markdown 2. a URL, filepath, or Xeno-Canto ID (in the form `xc123456`).\n",
    "#@markdown\n",
    "#@markdown Running the cell will display the example and allow you to select the\n",
    "#@markdown 5-second portion of it to use.\n",
    "#@markdown > **NOTE: If your example is too long, this can make the selection of\n",
    "#@markdown > the 5-second segment a bit more difficult.**\n",
    "\n",
    "agile.display_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHUJ_NwQWZNB",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Embed the query and retrieve most similar candidates\n",
    "#@markdown <font color='green'>\u2190 Run this cell after reading the instructions\n",
    "#@markdown below to embed the query and perform the search.</font>\n",
    "#@markdown\n",
    "#@markdown The next step is to generate an embedding for the 5-second example\n",
    "#@markdown and then compare it against the embeddings in the database to find\n",
    "#@markdown the most similar 5-second clips from your search dataset.\n",
    "#@markdown\n",
    "#@markdown You can leave the options below unchanged when first running this\n",
    "#@markdown cell.\n",
    "num_results = 10  #@param\n",
    "#@markdown This controls the number of search results to present. Larger numbers\n",
    "#@markdown allow to annotate more search results at a time, but going through\n",
    "#@markdown the results requires more annotator time.\n",
    "target_score = None  #@param\n",
    "#@markdown When leaving `target_score` to None, the clips being surfaced will be\n",
    "#@markdown the top `num_results` most similar clips with respect to the provided\n",
    "#@markdown query example. It can however be useful to retrieve embeddings with\n",
    "#@markdown different levels of similarity, for instance to get good \"negative\"\n",
    "#@markdown training examples to contrast against the \"positive\" matches. Running\n",
    "#@markdown this cell will display a histogram of scores like this one:\n",
    "#@markdown\n",
    "#@markdown <div><img src=\"https://storage.googleapis.com/chirp-public-bucket/esa-2024/logits_distribution.png\" width=\"300\"/></div>\n",
    "#@markdown\n",
    "#@markdown The x-axis represents some arbitrary similarity \"score\", and the y-axis\n",
    "#@markdown represents how many embeddings in the database share that score. In\n",
    "#@markdown the example above, a `target_score` of 0.0 would for instance retrieve\n",
    "#@markdown embeddings in the database whose similarity \"score\" is closest to 0.0\n",
    "#@markdown and which are therefore amongst the most dissimilar to the provided\n",
    "#@markdown example.\n",
    "\n",
    "agile.embed_query()\n",
    "\n",
    "agile.search_with_query(\n",
    "    num_results=num_results,\n",
    "    # When working with really large datasets it may be necessary for\n",
    "    # performance reasons to look at a smaller (random) subset of the database\n",
    "    # entries to perform the search. Replacing None with an integer argument\n",
    "    # would limit the search to a random subset of that size.\n",
    "    sample_size=None,\n",
    "    target_score=target_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCYqYibsnQFR",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Inspect and annotate the search results\n",
    "#@markdown <font color='green'>\u2190 Run this cell after reading the instructions\n",
    "#@markdown below to inspect and annotate the retrieved recordings in the\n",
    "#@markdown database.</font>\n",
    "#@markdown\n",
    "#@markdown We are now ready to look at our first search results. For each\n",
    "#@markdown example you can look at a spectrogram and listen to the audio, then\n",
    "#@markdown apply a positive label if it is a positive match or a negative label\n",
    "#@markdown if it's not a match.\n",
    "#@markdown\n",
    "#@markdown The label itself that will be applied is any string you specify via\n",
    "#@markdown the text form below and is up to you. For instance, you could apply\n",
    "#@markdown the `ybg` label for yellow-bellied glider vocalizations.\n",
    "query_label = 'ybg'  #@param {type:'string'}\n",
    "#@markdown Click the label below each recording to annotate it: click once to\n",
    "#@markdown turn it green (positive label), twice to turn it orange (negative\n",
    "#@markdown label), or leave it unclicked (or click a third times to reset) if\n",
    "#@markdown you don't want to apply any label to the recording (i.e., you don't\n",
    "#@markdown want to add it to your labelled training set at all).\n",
    "#@markdown\n",
    "#@markdown > **NOTE: loading the spectrograms can sometimes fail. If you see\n",
    "#@markdown > some examples that failed to load, try running the cell a second\n",
    "#@markdown > time before starting your labelling.**\n",
    "\n",
    "agile.display_search_results(query_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3sIkOqlXzKB",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Save the annotations { vertical-output: true }\n",
    "#@markdown <font color='green'>\u2190 Run this cell to save your annotations.</file>\n",
    "#@markdown\n",
    "#@markdown This will save the newly labelled examples to the database.\n",
    "\n",
    "agile.save_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlqyyuBNnQFR"
   },
   "source": [
    "Repeat the above cycle with a few different audio queries:\n",
    "\n",
    "1. Load the query audio.\n",
    "2. Embed the query and retrieve most similar candidates.\n",
    "3. Inspect and annotate the search results.\n",
    "4. Save the annotations.\n",
    "\n",
    "If your target species has multiple call types, it would be a good idea to search for at least one of each call type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o65wpjvyYft-"
   },
   "source": [
    "# 5. [Previous text block] Build a machine learning classifier model from the search results\n",
    "\n",
    "Now that we have labelled a number of our embedded audio clips in the search set, we have what we need to train and evaluate a classifier.\n",
    "\n",
    "This classifier is a statistical model. It is trained through an algorithm called Gradient Descent. This process requires setting some parameters.\n",
    "\n",
    "For the most part, you the default values for all of these will work fine. You might want to start by leaving the defaults, then experiment later to see if you can get any improvements.\n",
    "\n",
    "1. **Target Labels**: If you have put more than one class into your embeddings database, and you don't want to build the model to include all of these, list the ones you do want to include\n",
    "\n",
    "The following are hyperparameters of the gradient descent algorithm. The algorithm looks at 'batches' of examples and updates the model's parameters a little after each batch.\n",
    "\n",
    "2. **learning_rate**: How much to update by after each batch\n",
    "3. **batch_size**: How many examples to include in each batch (generally leave it pretty high and only reduce if you have RAM problems)\n",
    "4. **num_steps**: How many times to look at all the batches. When you train, you will see the \"loss\" decreasing as it learns. The num_steps should ideally be set so that arrives at the last step just after it stops improving.\n",
    "\n",
    "The following are to do with the labelled data inputs.\n",
    "\n",
    "5. **train_ratio**: A random subset of the labelled audio is not used to train the model, but instead is used to test the model. This is so we know roughly how well the model does on classifying examples that it has never seen before.\n",
    "3. **weak_neg_weight**: In your database we have a lot of audio, most of which is probably not your target. By taking some random clips from your unlabelled audio and treating them as negative examples, we can train on a wider variety of negative examples than what has been explicitly labelled as negative. However, because we don't know for sure that this process didn't choose a positive example by chance, we give each one less importance in the training.\n",
    "6. **weak_negatives_batch_size**: How many of these randomly chosen examples to include for each batch (on top of the number in the strongly labelled batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Build a machine learning classifier model from the search results\n",
    "\n",
    "Now that we have labelled a number of our embedded audio clips in the search set, we have what we need to train and evaluate a classifier.\n",
    "\n",
    "At a high level, you can think of our embeddings as points on a map. From that perspective, classifying audio clips as \"positive\" (match) or \"negative\" (not a match) can be thought of as figuring out \"territories\" on our map for positives and negatives. If an embedding for a particular audio clip falls into the \"positives\" territory, it is classified as a positive, and vice versa.\n",
    "\n",
    "To continue with the analogy, our territories are defined using \"capital cities\": we place a capital city for positives and one for for negatives on our map, and the territory that any point on our map belongs to is determined by which capital city the point is closest to.\n",
    "\n",
    "\"Training\" the classifier therefore reduces to finding the two points on our map at which to place the capital cities such that the territories they claim align with our labelled audio clips. In an ideal case, the capitals should be located such that all positively-labelled clips in our dataset are in the \"positives\" territory, and vice versa.\n",
    "\n",
    "The process by which we figure out the locations of our capital cities is beyond the scope of this tutorial, and the knobs one needs to tune to control the behavior of that process can be safely ignored and left to their default settings below.\n",
    "\n",
    "1. **Target Labels**: If you have put more than one class into your embeddings database, and you don't want to build the model to include all of these, list the ones you do want to include\n",
    "\n",
    "The following are hyperparameters of the gradient descent algorithm. The algorithm looks at 'batches' of examples and updates the model's parameters a little after each batch.\n",
    "\n",
    "2. **learning_rate**: How much to update by after each batch\n",
    "3. **batch_size**: How many examples to include in each batch (generally leave it pretty high and only reduce if you have RAM problems)\n",
    "4. **num_steps**: How many times to look at all the batches. When you train, you will see the \"loss\" decreasing as it learns. The num_steps should ideally be set so that arrives at the last step just after it stops improving.\n",
    "\n",
    "The following are to do with the labelled data inputs.\n",
    "\n",
    "5. **train_ratio**: A random subset of the labelled audio is not used to train the model, but instead is used to test the model. This is so we know roughly how well the model does on classifying examples that it has never seen before.\n",
    "3. **weak_neg_weight**: In your database we have a lot of audio, most of which is probably not your target. By taking some random clips from your unlabelled audio and treating them as negative examples, we can train on a wider variety of negative examples than what has been explicitly labelled as negative. However, because we don't know for sure that this process didn't choose a positive example by chance, we give each one less importance in the training.\n",
    "6. **weak_negatives_batch_size**: How many of these randomly chosen examples to include for each batch (on top of the number in the strongly labelled batch)"
   ],
   "metadata": {
    "id": "Yzr4dbdNLlJR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtsJkgcPYg6z"
   },
   "outputs": [],
   "source": [
    "#@title Classifier training. { vertical-output: true }\n",
    "\n",
    "#@markdown Set of labels to classify. If None, auto-populated from the DB.\n",
    "target_labels = None  #@param\n",
    "\n",
    "learning_rate = 1e-3  #@param\n",
    "num_steps = 101  #@param\n",
    "batch_size = 32  #@param\n",
    "\n",
    "train_ratio = 0.9  #@param\n",
    "weak_neg_weight = 0.05  #@param\n",
    "weak_negatives_batch_size = 16  #@param\n",
    "\n",
    "agile.train_classifier(target_labels, learning_rate, weak_neg_weight, num_steps, train_ratio, batch_size, weak_negatives_batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAKqXm2dnQFR"
   },
   "source": [
    "Now that we have a trained classifier, we can follow the same process as for the single example query. We search the database for more examples using the classifier, label them, then re-train the classifier with the new examples.\n",
    "\n",
    "The classifier outputs a score for each example in the search set. It believes anything above zero belongs to your target class, and anything below doesn't.\n",
    "\n",
    "When searching using the classifier, we can look for examples with the highest score by setting `target_score` to `None`.  This might get us more positive examples but these probably won't improve the classifier much, because they already have a high score. More useful is to search for those examples that the classifier is least sure about, by setting `target_score` to `0`.  Try setting the target score to None, 0 and possibly some other values depending on how many positive examples come back from each of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCtooq31nQFR"
   },
   "outputs": [],
   "source": [
    "#@title Review Classifier Results. { vertical-output: true }\n",
    "#@markdown Our target call-type label\n",
    "query_label = 'ybg'  #@param {type:'string'}\n",
    "#@markdown Number of results to retrieve.\n",
    "num_results = 10  #@param\n",
    "#@markdown Number of (randomly selected) database entries to search over.\n",
    "sample_size = None  #@param\n",
    "#@markdown When margin sampling, target this logit.\n",
    "target_score = None  #@param\n",
    "\n",
    "agile.search_with_classifier(query_label, num_results, sample_size, target_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3N6dzhetkG1"
   },
   "outputs": [],
   "source": [
    "agile.display_search_results(query_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEk15jw_B8xL"
   },
   "outputs": [],
   "source": [
    "#@title Save data labels. { vertical-output: true }\n",
    "\n",
    "agile.save_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iqGtCPknQFR"
   },
   "source": [
    "# Saving your classifier and running inference\n",
    "\n",
    "The trained classifier consists of the following elements\n",
    "1. The *weights* of the model (how to multiply and add the embedding values together to produce higher scores for the examples of the target class than other examples), which were learned during training.\n",
    "2. The model *bias* (how to shift the scores so that scores for positive examples are positive and vice-versa), also learned during training.\n",
    "3. The list of labels (class names) corresponding to the output scores\n",
    "4. Some metadata related to the model that created the embeddings, so that if the classifier is used on new audio, we make sure to embed in a compatible way.\n",
    "\n",
    "With this information, the classifier can be saved and used on other search sets later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHFaxBbhnQFR"
   },
   "outputs": [],
   "source": [
    "classifier_name = 'ybg_01'  #@param {type:'string'}\n",
    "\n",
    "classifier_path = pathlib.Path(config.models_folder) / f'{classifier_name}.json'\n",
    "\n",
    "classifier_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "agile.classifier.save(classifier_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2krifHJZnQFR"
   },
   "source": [
    "We can also run the model over all of the search dataset and save the results to a csv. Specify:\n",
    "1. The csv filename to save the results to\n",
    "2. The threshold. Anything above the threshold for the target labels will be saved. Typically this would be zero to save anything the classifier believes is the target class\n",
    "3. Which labels to include. Leave it as None to include all the labels you trained for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJMfJJ2nnQFR"
   },
   "outputs": [],
   "source": [
    "output_filename = 'ybg_output.csv'  #@param {type:'string'}\n",
    "\n",
    "threshold=0.0 #@param {type:'string'}\n",
    "\n",
    "# You can also specify a random subset of the dataset to run inference on, e.g. 0.5 for 50%\n",
    "subset = 0.1 #@param {type:'string'}\n",
    "\n",
    "# Which labels to include in the output file. If None, all labels are included.\n",
    "labels = None\n",
    "\n",
    "output_filepath = pathlib.Path(agile.config.predictions_folder) / output_filename\n",
    "pathlib.Path(output_filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "agile.run_inference(output_filepath, threshold=0.0, dataset=config.search_dataset_name, subset=subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TODO: remove cells in this section"
   ],
   "metadata": {
    "id": "QF6o2WW91NlC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from chirp.projects.agile2.convert_legacy import df_to_embeddings, extract_metadata\n",
    "\n",
    "embeddings_files = config.embeddings_folder\n",
    "source_map_fn = lambda x: x\n",
    "\n",
    "if (Path(embeddings_files) / 'filelist.json').exists():\n",
    "  parquet_filepaths = [Path(embeddings_files) / 'embeddings' / Path(str(f['site_id'])) / Path(str(f['id'])) / 'embeddings.parquet'\n",
    "                    for f in json.load(open(Path(embeddings_files) / 'filelist.json'))]\n",
    "else:\n",
    "  parquet_filepaths = None\n",
    "\n",
    "def generator():\n",
    "  for fp in parquet_filepaths:\n",
    "      try:\n",
    "          df = pd.read_parquet(fp)\n",
    "          embeddings_table = df_to_embeddings(df)\n",
    "          embeddings, filename, timestamp_s, embedding_shape = extract_metadata(embeddings_table)\n",
    "          filename = source_map_fn(filename)\n",
    "          yield {\n",
    "              'filename': filename.encode(),\n",
    "              'timestamp_s': timestamp_s,\n",
    "              'embedding': embeddings,\n",
    "              'embedding_shape': embedding_shape\n",
    "          }\n",
    "      except Exception as e:\n",
    "          print(f\"Unexpected error on {fp}: {e}\")\n",
    "          continue"
   ],
   "metadata": {
    "id": "FD2PBZEU9MTb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for _ in tqdm(generator(), total=len(parquet_filepaths)):\n",
    "  pass"
   ],
   "metadata": {
    "id": "nm2k5H2RAhIb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def df_to_embeddings(df):\n"
   ],
   "metadata": {
    "id": "2htFmVnEBDJP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def generator():\n",
    "  ds = pyarrow.parquet.ParquetDataset(Path(embeddings_files) / 'embeddings')\n",
    "  for fragment in ds.fragments:\n",
    "      try:\n",
    "          df = fragment.to_table().to_pandas()\n",
    "          embeddings_table = df_to_embeddings(df)\n",
    "          yield True\n",
    "          embeddings, filename, timestamp_s, embedding_shape = extract_metadata(embeddings_table)\n",
    "          filename = source_map_fn(filename)\n",
    "          yield {\n",
    "              'filename': filename.encode(),\n",
    "              'timestamp_s': timestamp_s,\n",
    "              'embedding': embeddings,\n",
    "              'embedding_shape': embedding_shape\n",
    "          }\n",
    "      except Exception as e:\n",
    "          print(f\"Unexpected error on {fp}: {e}\")\n",
    "          continue"
   ],
   "metadata": {
    "id": "UoIwtJZk-U4Q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = fragment.to_table().to_pandas()"
   ],
   "metadata": {
    "id": "taGElunHBQNF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.columns"
   ],
   "metadata": {
    "id": "dRS07ASvBTry"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for _ in tqdm(generator(), total=len(parquet_filepaths)):\n",
    "  pass"
   ],
   "metadata": {
    "id": "G-3kYzvy93Oq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pyarrow\n",
    "import pyarrow.parquet\n",
    "\n",
    "ds = pyarrow.parquet.ParquetDataset(config.embeddings_folder / 'embeddings')"
   ],
   "metadata": {
    "id": "-_SURLj4w02L"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "table = ds._dataset.to_table(batch_readahead=64, fragment_readahead=8)"
   ],
   "metadata": {
    "id": "Mq32gqoj5GYF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "def count_files(directory):\n",
    "  \"\"\"\n",
    "  Recursively counts the number of files in a directory.\n",
    "\n",
    "  Args:\n",
    "    directory: The path to the directory to count files in.\n",
    "\n",
    "  Returns:\n",
    "    The total number of files in the directory and its subdirectories.\n",
    "  \"\"\"\n",
    "  count = 0\n",
    "  for root, _, files in os.walk(directory):\n",
    "    count += len(files)\n",
    "  return count\n",
    "\n",
    "count_files(config.embeddings_folder / 'embeddings')"
   ],
   "metadata": {
    "id": "W3I51Vbh2cK2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pdb; pdb.set_trace()\n",
    "\n",
    "ds.read()"
   ],
   "metadata": {
    "id": "iVx8bxCk3Z8V"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_rows = sum(fragment.metadata.num_rows for fragment in ds.fragments)\n",
    "\n",
    "with tqdm(total=num_rows) as pbar:\n",
    "  for batch in ds._dataset.to_batches():\n",
    "    pbar.update(batch.num_rows)"
   ],
   "metadata": {
    "id": "dy_UE912w9fk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_rows = sum(fragment.metadata.num_rows for fragment in ds.fragments)\n",
    "\n",
    "with tqdm(total=num_rows) as pbar:\n",
    "  for fragment in ds.fragments:\n",
    "    table = fragment.to_table()\n",
    "    pbar.update(table.num_rows)"
   ],
   "metadata": {
    "id": "CJK5GmNC8XxG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "table.num_rows"
   ],
   "metadata": {
    "id": "JXSzIhMb8jqR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "batch.drop_columns([\"source\", \"channel\", \"offset\"]).to_tensor().to_numpy().shape"
   ],
   "metadata": {
    "id": "qlQzb8es7bMD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "def convert_parquet(\n",
    "    parquet_folder: str,\n",
    "    dataset_name: str,\n",
    "    parquet_filepaths: list = None,\n",
    "    max_count: int = -1,\n",
    "    prefetch: int = 128,\n",
    "    source_map_fn = lambda x: x,\n",
    "    **kwargs,\n",
    "):\n",
    "  ds = pyarrow.parquet.ParquetDataset(parquet_folder / 'embeddings')\n",
    "\n",
    "\n",
    "\n",
    "def create_database(self, embeddings_files):\n",
    "  \"\"\"\n",
    "  Checks if a database already exists at the specified path, and if not, creates one from the files at the location\n",
    "  specified. If it does allows the user to delete and then creates the database.\n",
    "  \"\"\"\n",
    "\n",
    "  db_path = Path(self.config.db_path)\n",
    "  db_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "  def create_db():\n",
    "\n",
    "    if (Path(embeddings_files) / 'filelist.json').exists():\n",
    "      parquet_filepaths = [Path(embeddings_files) / 'embeddings' / Path(str(f['site_id'])) / Path(str(f['id'])) / 'embeddings.parquet'\n",
    "                        for f in json.load(open(Path(embeddings_files) / 'filelist.json'))]\n",
    "    else:\n",
    "      parquet_filepaths = None\n",
    "\n",
    "    if self.config.max_embeddings_count:\n",
    "      max_count = self.config.max_embeddings_count\n",
    "    else:\n",
    "      max_count = -1\n",
    "\n",
    "    if not self.config.search_dataset_name:\n",
    "      print('search_dataset_name not set in config. Please set it before creating the database.')\n",
    "      return\n",
    "\n",
    "    print(f'creating db at {db_path.resolve()}')\n",
    "    db = convert_parquet(parquet_folder = Path(embeddings_files),\n",
    "                         parquet_filepaths = parquet_filepaths,\n",
    "                         dataset_name = self.config.search_dataset_name,\n",
    "                         max_count=max_count,\n",
    "                         db_path=db_path)\n",
    "    print(f'db created at {db_path.resolve()} with {db.count_embeddings()} embeddings.')\n",
    "\n",
    "  if db_path.exists():\n",
    "\n",
    "    print(f\"DB path already exists at {db_path.resolve()}. (Initialize the DB to see details)\")\n",
    "\n",
    "    button = widgets.Button(description=\"Delete Existing Database and create it again?\",\n",
    "                            layout=widgets.Layout(width='auto'))\n",
    "    def on_button_click(b):\n",
    "        db_path.unlink()\n",
    "        (db_path.parent / Path(db_path.name + \"-shm\")).unlink()\n",
    "        (db_path.parent / Path(db_path.name + \"-wal\")).unlink()\n",
    "        create_db()\n",
    "    button.on_click(on_button_click)\n",
    "    display(button)\n",
    "\n",
    "  else:\n",
    "    create_db()"
   ],
   "metadata": {
    "id": "dteTevXJuNNH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### End TODO"
   ],
   "metadata": {
    "id": "Og_96HUN1UTs"
   }
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "collapsed_sections": [
    "QF6o2WW91NlC"
   ]
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
