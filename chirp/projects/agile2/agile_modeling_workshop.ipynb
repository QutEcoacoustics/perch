{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lp0RzLu9KerR"
   },
   "source": [
    "# Build Your Own Call Recogniser\n",
    "\n",
    "_Integrating Passive Acoustic Monitoring with AI for Scalable Biodiversity Tracking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LId8il9BKiTU"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to the agile modelling Python notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yobMfr-RKpzX"
   },
   "source": [
    "### What is a Python notebook?\n",
    "\n",
    "A Python notebook allows you to run Python code in a Python environment. If you are running this notebook in Google Colab, the Python notebook is running in a virtual machine in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkKLhINiKsq8"
   },
   "source": [
    "### Do I need to be familiar with Python?\n",
    "\n",
    "No, you do not need to be familiar with Python to work through the notebook. You will interact with the notebook via UI elements such as text boxes, dropdown menus, and buttons.\n",
    "\n",
    "In fact, most of the Python code in the notebook is hidden by default to allow you to focus on the agile modelling workflow itself. If you are curious to look behind the curtain, you can click a code cell's \"Show code\" button like so:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://storage.googleapis.com/chirp-public-bucket/esa-2024/reveal_code.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbqUAfBVnQFO"
   },
   "source": [
    "### Notebook overview\n",
    "\n",
    "In this notebook, we will use a process called \"[agile modeling](https://arxiv.org/abs/2302.12948)\" to build and incrementally improve a classifier for acoustic analysis, starting from a single classified example. The process uses embeddings provided by the [Perch model](https://www.kaggle.com/models/google/bird-vocalization-classifier). These are the steps we will take:\n",
    "\n",
    "1. Setup\n",
    "2. Configure the Perch agile modelling modules\n",
    "3. Create a database of embeddings\n",
    "4. Search for recordings similar to the annotator-provided example\n",
    "5. Build a machine learning classifier model from the search results\n",
    "6. Search your recordings based on the results of the classifier\n",
    "7. Improve your classifier further using these search results\n",
    "\n",
    "The agile modelling process is described in more detail in [these slides](https://docs.google.com/presentation/d/e/2PACX-1vTfvoBvCi_V72s0RiIcmFNdnZDcPDCDl-omBbODJ3sz3_IxD5kd1zJjd-J8AR7PE_DgxO-FWDjyP7Mb/pub?start=false&loop=false&delayms=3000&slide=id.g2d63d0c2ccf_0_3915) and in the following diagram:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://storage.googleapis.com/chirp-public-bucket/esa-2024/agile_modelling_workflow.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNdr15dmLvk5"
   },
   "source": [
    "## 1. Setup\n",
    "\n",
    "You are running this notebook in a Python environment. We need to add the Perch package to this environment. You only need to do this once, however if you are running this notebook in the cloud on Google Colab, your session is only ephemaral. You will need to rerun his this cell after disconnecting.\n",
    "\n",
    "> **NOTE: The session needs to be restarted after this step.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tiPc5ofSLzj3"
   },
   "outputs": [],
   "source": [
    "#@title Install the `perch` package and import requirements\n",
    "#@markdown <font color='green'>\u2190 Run this cell to install the Perch package and\n",
    "#@markdown import requirements.</font>\n",
    "#@markdown\n",
    "#@markdown After running this cell for the first time, you need to restart your\n",
    "#@markdown session in order for the changes made by installing the Perch package\n",
    "#@markdown take effect.\n",
    "#@markdown\n",
    "#@markdown You should be automatically prompted for a session restart, but if\n",
    "#@markdown you are not please manually restart the session like so:\n",
    "#@markdown\n",
    "#@markdown <div>\n",
    "#@markdown <img src=\"https://storage.googleapis.com/chirp-public-bucket/esa-2024/restart_session.png\" width=\"300\"/>\n",
    "#@markdown </div>\n",
    "#@markdown\n",
    "#@markdown > **NOTE: after restarting the session, you need to run this cell\n",
    "#@markdown > again.**\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "from IPython import display as ipython_display\n",
    "\n",
    "display(ipython_display.Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 128})'''))\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "try:\n",
    "  from chirp.projects.agile2 import agile_modeling_state\n",
    "\n",
    "  agile2_config = agile_modeling_state.agile2_config\n",
    "  agile2_state = agile_modeling_state.agile2_state\n",
    "  download_embeddings = agile_modeling_state.download_embeddings\n",
    "  Helpers = agile_modeling_state.Helpers\n",
    "\n",
    "  import dotenv\n",
    "\n",
    "  find_dotenv = dotenv.find_dotenv\n",
    "  load_dotenv = dotenv.load_dotenv\n",
    "\n",
    "  if not __import__('numpy').__version__.startswith('1.24'):\n",
    "    print(\n",
    "        'Make sure you have restarted the session after installing Perch,'\n",
    "        ' following the instructions above.'\n",
    "    )\n",
    "except ImportError:\n",
    "  !pip install git+https://github.com/QutEcoacoustics/perch.git@7726d70556e00ecf7328ac91e572010f9ce9cb03 python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_2yCNFHBNOwF"
   },
   "outputs": [],
   "source": [
    "#@title Link to Google Drive\n",
    "#@markdown <font color='green'>\u2190 Run this cell to link to Google Drive.</font>\n",
    "#@markdown\n",
    "#@markdown We will need somewhere to read and write files. This colab environment\n",
    "#@markdown where the notebook is running does not persist between sessions, so\n",
    "#@markdown we will link to google drive for access to persistent storage.\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "except:\n",
    "    print(\"colab not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnsSbQa5nQFP"
   },
   "source": [
    "## 2. Configure the Perch agile modelling modules\n",
    "\n",
    "Here we set some configuration for names and local filepaths and initialize our agile modeling workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "xbQDsw0hnQFP"
   },
   "outputs": [],
   "source": [
    "#@title Configure the agile modelling workflow\n",
    "#@markdown <font color='green'>\u2190 Run this cell to configure the agile modelling\n",
    "#@markdown workflow.</font>\n",
    "annotator_id = \"phil\"  # @param {type:'string'}\n",
    "#@markdown The purpose of the `annotator_id` variable above is to keep track of\n",
    "#@markdown who provided each annotation when collaborating across multiple\n",
    "#@markdown annotators.\n",
    "\n",
    "# This is the location on google drive that this tutorial will use to save data.\n",
    "# If you followed the above instructions for creating a shortcut to the Drive\n",
    "# folder, you should be able to navigate to this directory in the left hand\n",
    "# \"Files\" menu in this Colab (indicated by the Folder icon on the far left\n",
    "# menu).\n",
    "working_folder = pathlib.Path('/content/drive/My Drive/esa2024_data')\n",
    "\n",
    "config = agile2_config(\n",
    "    db_path=working_folder / 'db' / 'db.sqlite',\n",
    "    annotator_id=annotator_id,\n",
    "    # For this tutorial we arbitrarily choose to name our search dataset\n",
    "    # \"search_set\", however if you manage multiple search datasets you will have\n",
    "    # to assign each a unique name via this configuration.\n",
    "    search_dataset_name=\"search_set\",\n",
    "    embeddings_folder=pathlib.Path('/content/embeddings'),\n",
    "    labeled_examples_folder=working_folder / 'labeled_examples',\n",
    "    models_folder=working_folder / 'models',\n",
    "    predictions_folder=working_folder / 'predictions',\n",
    ")\n",
    "\n",
    "config.labeled_examples_folder.mkdir(exist_ok=True, parents=True)\n",
    "config.embeddings_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "config.baw_config['domain'] = 'api.ecosounds.org'\n",
    "\n",
    "agile = agile2_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fSI2gNN8HV-c"
   },
   "outputs": [],
   "source": [
    "#@title Configure access to your Ecosounds project\n",
    "#@markdown <font color='green'>\u2190 Run this cell after reading the instructions\n",
    "#@markdown below to configure access to your\n",
    "#@markdown Ecosounds project.</font>\n",
    "#@markdown\n",
    "#@markdown > **NOTE: If you are _not_ using your own data for the tutorial, you can skip this step.**\n",
    "#@markdown\n",
    "#@markdown We will be loading audio from [Ecosounds](https://www.ecosounds.org),\n",
    "#@markdown an online repository of ecoacoustic recordings. If working with a\n",
    "#@markdown private Ecosounds project, we need to provide the Ecosounds *auth\n",
    "#@markdown token* associated with your ecosounds account.\n",
    "#@markdown\n",
    "#@markdown To find your ecosounds token, go to https://www.ecosounds.org/my_account\n",
    "#@markdown and in the bottom left, click on the button to copy the token like so:\n",
    "#@markdown\n",
    "#@markdown <div><img src=\"https://storage.googleapis.com/chirp-public-bucket/esa-2024/ecosounds_token.png\" width=\"800\"/></div>\n",
    "#@markdown\n",
    "#@markdown Because this is a secret, we should avoid saving it in plain text in\n",
    "#@markdown the notebook, so we will set it up in an environment variable.\n",
    "#@markdown Depending on where you are running this notebook, do one of the\n",
    "#@markdown following (if you are not sure, just run this cell and it will tell you).\n",
    "#@markdown\n",
    "#@markdown > **Colab**\n",
    "#@markdown\n",
    "#@markdown <div><img src=\"https://storage.googleapis.com/chirp-public-bucket/esa-2024/colab_secrets.png\" width=\"500\"/></div>\n",
    "#@markdown\n",
    "#@markdown 1. On the right, click on the key icon to open the \"secrets\" tab.\n",
    "#@markdown 2. Click \"Add new secret\"\n",
    "#@markdown 3. Under \"Name\" put the text `BAW_AUTH_TOKEN` (without quotes)\n",
    "#@markdown 4. Under \"Value\" paste the token you copied from Ecosounds\n",
    "#@markdown\n",
    "#@markdown > **Jupyter, running locally**\n",
    "#@markdown\n",
    "#@markdown 1. In the working directory create a file named `.env`\n",
    "#@markdown    - The working directory is probably the directory where you launched the notebook. If you are not sure, run the cell below and it will tell you.\n",
    "#@markdown 2. In this .env file, put the line `BAW_AUTH_TOKEN=abc123xyz` (replace `abc123xyz` with the token you copied from Ecosounds)\n",
    "auth_token = None\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    auth_token = userdata.get('BAW_AUTH_TOKEN')\n",
    "    print(\"Got auth token from colab secrets\")\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    env_file = find_dotenv()\n",
    "    if not env_file:\n",
    "        print(f\"No .env file found in the working directory {os.getcwd()}. \\nFollow the local Jupyter instructions above to create one.\")\n",
    "    else:\n",
    "        load_dotenv(override=True)\n",
    "        auth_token = os.getenv('BAW_AUTH_TOKEN')\n",
    "        if auth_token:\n",
    "            print(f\"Got auth token from .env file {env_file}\")\n",
    "        else:\n",
    "            print(\"BAW_AUTH_TOKEN env variable not found in your .env file. Follow the local Jupyter instructions above to set it\")\n",
    "\n",
    "except userdata.SecretNotFoundError:\n",
    "        print(\"No BAW_AUTH_TOKEN secret found, please follow the colab instructions above to set it\")\n",
    "\n",
    "if auth_token:\n",
    "    if config.baw_config.get('auth_token'):\n",
    "        print(\"Overwriting config auth token with new value\")\n",
    "    config.baw_config['auth_token'] = auth_token\n",
    "elif config.baw_config.get('auth_token'):\n",
    "    print(\"Auth token not loaded, but already in config\")\n",
    "else:\n",
    "    print(\"No auth token set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxukSWBCnQFQ"
   },
   "source": [
    "## 3. Create a database of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "UWYxjkG2nQFQ"
   },
   "outputs": [],
   "source": [
    "#@title Download audio embeddings locally\n",
    "#@markdown <font color='green'>\u2190 Run this cell after making a choice below to\n",
    "#@markdown download audio embeddings.</font>\n",
    "#@markdown\n",
    "#@markdown Choose a publicly-available option in the dropdown or type in the\n",
    "#@markdown name of the dataset you provided.\n",
    "dataset_name = \"yellow_bellied_glider\" # @param [\"yellow_bellied_glider\",\"gympie\",\"gympie_small\",\"forty_spotted_pardalote\"] {\"allow-input\":true}\n",
    "\n",
    "download_from_gcp = dataset_name in [\n",
    "    \"yellow_bellied_glider\",\n",
    "    \"gympie\",\n",
    "    \"gympie_small\",\n",
    "    \"forty_spotted_pardalote\",\n",
    "]\n",
    "\n",
    "download_embeddings(\n",
    "    dataset_name,\n",
    "    config.embeddings_folder,\n",
    "    download_from_gcp=download_from_gcp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-6yH31NqnQFQ"
   },
   "outputs": [],
   "source": [
    "#@title Create a database of embeddings\n",
    "#@markdown <font color='green'>\u2190 Run this cell to create the embeddings database.</font>\n",
    "#@markdown\n",
    "#@markdown The database links labels to embeddings so we can train our classifier.\n",
    "\n",
    "agile.create_database(config.embeddings_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "njS1zcv-nQFQ"
   },
   "outputs": [],
   "source": [
    "#@title Initialize the agile modelling workflow\n",
    "#@markdown <font color='green'>\u2190 Run this cell to initialize the workflow.\n",
    "\n",
    "agile.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFRHcrs9cWgl"
   },
   "source": [
    "### What is an embedding?\n",
    "\n",
    "Think of an embedding as a summary of the audio waveform in the form of an array of numbers. In the context of deep learning (which Perch relies on), that array of numbers is obtained by passing a _spectrogram_ representation of the audio through a deep neural network.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://storage.googleapis.com/chirp-public-bucket/esa-2024/what_is_an_embedding.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "The exact details can be safely ignored for the purpose of this tutorial, but the neural network was constructed and trained in such a way that the relationship between the embeddings it outputs tend to be consistent with the semantic relationships between the audio waveforms (e.g., passing two different vocalizations for the same animal species through the neural network will produce similar embeddings).\n",
    "\n",
    "We will exploit this property in our agile modelling workflow.\n",
    "\n",
    "The first step in the workflow is therefore to compute an embedding with Perch for every possible 5-seconds audio clip extracted from our audio recordings. In the interest of time, those embeddings have already been computed and the work needed for this step is to download the embeddings locally and format them appropriately.\n",
    "\n",
    "If you have uploaded audio for this workshop, you will have been given a name to use for that search set.\n",
    "\n",
    "Otherwise, you can use one of the following public search sets:\n",
    "\n",
    "1. `yellow_bellied_glider`\n",
    "2. `gympie`\n",
    "3. `gympie_small` (a smaller dataset that takes less time to prepreocess and search through)\n",
    "4. `forty_spotted_pardalote`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVdLJJd9gnjo"
   },
   "source": [
    "## 4. Search for recordings similar to the annotator-provided example\n",
    "\n",
    "Here, we take a single example and find the examples in our search set which most closely match that example. This is a way to get started with a labelled training set.\n",
    "\n",
    "You have three options to provide examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "gK6hsJDInQFQ"
   },
   "outputs": [],
   "source": [
    "#@title A. Copying your own files in Google Drive\n",
    "#@markdown <font color='orange'>\u2190 [OPTIONAL] Run this cell to list audio files\n",
    "#@markdown in your mounted Google Drive folder.</font>\n",
    "#@markdown\n",
    "#@markdown If you have short examples of your target call, copy them into the\n",
    "#@markdown `config.labeled_examples_folder` directory and then run this cell to\n",
    "#@markdown check that they are accessible.\n",
    "#@markdown\n",
    "#@markdown If you are unsure where that is, running this cell will display the\n",
    "#@markdown path to `config.labeled_examples_folder`.\n",
    "\n",
    "audio_files = Helpers.list_audio_files(config.labeled_examples_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmBOY5HrFAyO"
   },
   "source": [
    "### B. Use the examples provided in our shared Google Drive folder\n",
    "\n",
    "We also have some labelled examples in a shared Google Drive folder. If you want to use those:\n",
    "\n",
    "- Navigate to the shared data [Google Drive folder](https://drive.google.com/drive/folders/1SQi-VunCpnqrPcQpaDrt2-VzVJigZvU9).\n",
    "- Click the dropdown menu labeled `labeled_examples`.\n",
    "- Select `Organize` -> `Add shortcut`.\n",
    "- Choose somewhere in your Google Drive to add the shortcut. You will use the path to this location later.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://storage.googleapis.com/chirp-public-bucket/esa-2024/shared_labeled_examples.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtE3uy-TnQFQ"
   },
   "source": [
    "### C. Provide a URL or Xeno Canto ID\n",
    "\n",
    "You can also directly provide a URL pointing to the example or a Xeno-Canto ID in the form `xc123456`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7ig3L5dsy3mr"
   },
   "outputs": [],
   "source": [
    "#@title Load the query audio { vertical-output: true }\n",
    "#@markdown <font color='green'>\u2190 Run this cell after reading the instructions\n",
    "#@markdown below to load the query audio.</font>\n",
    "query = '1' # @param {type:'string'}\n",
    "#@markdown The `query` above can be\n",
    "#@markdown 1. one of the integer indices listed after running the cell for\n",
    "#@markdown    option A above; or\n",
    "#@markdown 2. a URL, filepath, or Xeno-Canto ID (in the form `xc123456`).\n",
    "#@markdown\n",
    "#@markdown Running the cell will display the example and allow you to select the\n",
    "#@markdown 5-second portion of it to use.\n",
    "#@markdown > **NOTE: If your example is too long, this can make the selection of\n",
    "#@markdown > the 5-second segment a bit more difficult.**\n",
    "\n",
    "agile.display_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "iHUJ_NwQWZNB"
   },
   "outputs": [],
   "source": [
    "#@title Embed the query and retrieve most similar candidates\n",
    "#@markdown <font color='green'>\u2190 Run this cell after reading the instructions\n",
    "#@markdown below to embed the query and perform the search.</font>\n",
    "#@markdown\n",
    "#@markdown The next step is to generate an embedding for the 5-second example\n",
    "#@markdown and then compare it against the embeddings in the database to find\n",
    "#@markdown the most similar 5-second clips from your search dataset.\n",
    "#@markdown\n",
    "#@markdown You can leave the options below unchanged when first running this\n",
    "#@markdown cell.\n",
    "num_results = 10  #@param\n",
    "#@markdown This controls the number of search results to present. Larger numbers\n",
    "#@markdown allow to annotate more search results at a time, but going through\n",
    "#@markdown the results requires more annotator time.\n",
    "target_score = None  #@param\n",
    "#@markdown When leaving `target_score` to None, the clips being surfaced will be\n",
    "#@markdown the top `num_results` most similar clips with respect to the provided\n",
    "#@markdown query example. It can however be useful to retrieve embeddings with\n",
    "#@markdown different levels of similarity, for instance to get good \"negative\"\n",
    "#@markdown training examples to contrast against the \"positive\" matches. Running\n",
    "#@markdown this cell will display a histogram of scores like this one:\n",
    "#@markdown\n",
    "#@markdown <div><img src=\"https://storage.googleapis.com/chirp-public-bucket/esa-2024/logits_distribution.png\" width=\"300\"/></div>\n",
    "#@markdown\n",
    "#@markdown The x-axis represents some arbitrary similarity \"score\", and the y-axis\n",
    "#@markdown represents how many embeddings in the database share that score. In\n",
    "#@markdown the example above, a `target_score` of 0.0 would for instance retrieve\n",
    "#@markdown embeddings in the database whose similarity \"score\" is closest to 0.0\n",
    "#@markdown and which are therefore amongst the most dissimilar to the provided\n",
    "#@markdown example.\n",
    "\n",
    "agile.embed_query()\n",
    "\n",
    "agile.search_with_query(\n",
    "    num_results=num_results,\n",
    "    # When working with really large datasets it may be necessary for\n",
    "    # performance reasons to look at a smaller (random) subset of the database\n",
    "    # entries to perform the search. Replacing None with an integer argument\n",
    "    # would limit the search to a random subset of that size.\n",
    "    sample_size=None,\n",
    "    target_score=target_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OCYqYibsnQFR"
   },
   "outputs": [],
   "source": [
    "#@title Inspect and annotate the search results\n",
    "#@markdown <font color='green'>\u2190 Run this cell after reading the instructions\n",
    "#@markdown below to inspect and annotate the retrieved recordings in the\n",
    "#@markdown database.</font>\n",
    "#@markdown\n",
    "#@markdown We are now ready to look at our first search results. For each\n",
    "#@markdown example you can look at a spectrogram and listen to the audio, then\n",
    "#@markdown apply a positive label if it is a positive match or a negative label\n",
    "#@markdown if it's not a match.\n",
    "#@markdown\n",
    "#@markdown The label itself that will be applied is any string you specify via\n",
    "#@markdown the text form below and is up to you. For instance, you could apply\n",
    "#@markdown the `ybg` label for yellow-bellied glider vocalizations.\n",
    "query_label = 'ybg'  #@param {type:'string'}\n",
    "#@markdown Click the label below each recording to annotate it: click once to\n",
    "#@markdown turn it green (positive label), twice to turn it orange (negative\n",
    "#@markdown label), or leave it unclicked (or click a third times to reset) if\n",
    "#@markdown you don't want to apply any label to the recording (i.e., you don't\n",
    "#@markdown want to add it to your labelled training set at all).\n",
    "#@markdown\n",
    "#@markdown > **NOTE: loading the spectrograms can sometimes fail. If you see\n",
    "#@markdown > some examples that failed to load, try running the cell a second\n",
    "#@markdown > time before starting your labelling.**\n",
    "\n",
    "agile.display_search_results(query_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "G3sIkOqlXzKB"
   },
   "outputs": [],
   "source": [
    "#@title Save the annotations { vertical-output: true }\n",
    "#@markdown <font color='green'>\u2190 Run this cell to save your annotations.</file>\n",
    "#@markdown\n",
    "#@markdown This will save the newly labelled examples to the database.\n",
    "\n",
    "agile.save_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlqyyuBNnQFR"
   },
   "source": [
    "Repeat the above cycle with a few different audio queries:\n",
    "\n",
    "1. Load the query audio.\n",
    "2. Embed the query and retrieve most similar candidates.\n",
    "3. Inspect and annotate the search results.\n",
    "4. Save the annotations.\n",
    "\n",
    "If your target species has multiple call types, it would be a good idea to search for at least one of each call type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yzr4dbdNLlJR"
   },
   "source": [
    "## 5. Build a machine learning classifier model from the search results\n",
    "\n",
    "Now that we have labelled a number of our embedded audio clips in the search set, we have what we need to train and evaluate a classifier.\n",
    "\n",
    "At a high level, you can think of our embeddings as points on a map. From that perspective, classifying audio clips as \"positive\" (match) or \"negative\" (not a match) can be thought of as figuring out \"territories\" on our map for positives and negatives. If an embedding for a particular audio clip falls into the \"positives\" territory, it is classified as a positive, and vice versa.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://storage.googleapis.com/chirp-public-bucket/esa-2024/classification.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "To continue with the analogy, our territories are defined using \"capital cities\": we place a capital city for positives and one for for negatives on our map, and the territory that any point on our map belongs to is determined by which capital city the point is closest to.\n",
    "\n",
    "\"Training\" the classifier therefore reduces to finding the two points on our map at which to place the capital cities such that the territories they claim align with our labelled audio clips. In an ideal case, the capitals should be located such that all positively-labelled clips in our dataset are in the \"positives\" territory, and vice versa.\n",
    "\n",
    "The process by which we figure out the locations of our capital cities is beyond the scope of this tutorial, and the knobs one needs to tune to control the behavior of that process can be safely ignored and left to their default settings below. Refer to the short explanation under each parameter for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qtsJkgcPYg6z"
   },
   "outputs": [],
   "source": [
    "#@title Train the classifier { vertical-output: true }\n",
    "#@markdown <font color='green'>\u2190 Run this cell to train the classifier.</file>\n",
    "#@markdown\n",
    "target_labels = None  #@param\n",
    "#@markdown The set of labels to classify. If None, auto-populated from the\n",
    "#@markdown database. If you have put more than one class into your embeddings\n",
    "#@markdown database, and you don't want to build the model to include all of\n",
    "#@markdown these, list the ones you do want to include\n",
    "#@markdown\n",
    "#@markdown *The following impact the procedure by which the \"capital city locations\"\n",
    "#@markdown are computed. This is an iterative procedure.*\n",
    "learning_rate = 1e-3  #@param\n",
    "#@markdown How much to update the locations at each step.\n",
    "num_steps = 101  #@param\n",
    "#@markdown How many steps to do.\n",
    "batch_size = 32  #@param\n",
    "#@markdown How many labelled examples to use at each step to determine an update\n",
    "#@markdown direction.\n",
    "#@markdown\n",
    "#@markdown *The following are to do with the labelled data inputs:*\n",
    "train_ratio = 0.9  #@param\n",
    "#@markdown A random subset of the labelled audio is not used to train the model,\n",
    "#@markdown but instead is used to test the model. This is so we know roughly how\n",
    "#@markdown well the model does on classifying examples that it has never seen before.\n",
    "weak_neg_weight = 0.05  #@param\n",
    "#@markdown In your database we have a lot of audio, most of which is probably\n",
    "#@markdown not your target. By taking some random clips from your unlabelled\n",
    "#@markdown audio and treating them as negative examples, we can train on a wider\n",
    "#@markdown variety of negative examples than what has been explicitly labelled\n",
    "#@markdown as negative. However, because we don't know for sure that this process\n",
    "#@markdown didn't choose a positive example by chance, we give each one less\n",
    "#@markdown importance in the training.\n",
    "weak_negatives_batch_size = 16  #@param\n",
    "#@markdown How many of these randomly chosen examples to include for each batch\n",
    "#@markdown (on top of the number in the strongly labelled batch).\n",
    "\n",
    "agile.train_classifier(target_labels, learning_rate, weak_neg_weight, num_steps, train_ratio, batch_size, weak_negatives_batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAKqXm2dnQFR"
   },
   "source": [
    "## 6. Search your recordings based on the results of the classifier\n",
    "\n",
    "Now that we have a trained classifier, we can follow the same process as for the single example query. We search the database for more examples using the classifier, label them, then re-train the classifier with the new examples.\n",
    "\n",
    "The classifier outputs a score for each example in the search set. Large positive/negative values mean the classifier predicts a positive/negative label with strong confidence, whereas values around zero mean the classifier's confidence is low.\n",
    "\n",
    "When searching using the classifier, we can look for examples with the highest score by setting `target_score` to `None`.  This might get us more positive examples but these probably won't improve the classifier much, because they already have a high score. More useful is to search for those examples that the classifier is least sure about, by setting `target_score` to `0`.  Try setting the target score to None, 0 and possibly some other values depending on how many positive examples come back from each of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OCtooq31nQFR"
   },
   "outputs": [],
   "source": [
    "#@title Search the database using the classifier { vertical-output: true }\n",
    "#@markdown <font color='green'>\u2190 Run this cell to search the database using the classifier.</file>\n",
    "#@markdown\n",
    "#@markdown Refer to the _Embed the query and retrieve most similar candidates_\n",
    "#@markdown and _Inspect and annotate the search results_ steps for more\n",
    "#@markdown information on the parameters below.\n",
    "query_label = 'ybg'  #@param {type:'string'}\n",
    "num_results = 10  #@param\n",
    "target_score = None  #@param\n",
    "\n",
    "agile.search_with_classifier(\n",
    "    target_label=query_label,\n",
    "    num_results=num_results,\n",
    "    # When working with really large datasets it may be necessary for\n",
    "    # performance reasons to look at a smaller (random) subset of the database\n",
    "    # entries to perform the search. Replacing None with an integer argument\n",
    "    # would limit the search to a random subset of that size.\n",
    "    sample_size=None,\n",
    "    target_score=target_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "a3N6dzhetkG1"
   },
   "outputs": [],
   "source": [
    "#@title Inspect and annotate the search results\n",
    "#@markdown <font color='green'>\u2190 Run this cell to inspect and annotate the\n",
    "#@markdown retrieved recordings in the database.</font>\n",
    "agile.display_search_results(query_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "kEk15jw_B8xL"
   },
   "outputs": [],
   "source": [
    "#@title Save the annotations { vertical-output: true }\n",
    "#@title Save the annotations { vertical-output: true }\n",
    "#@markdown <font color='green'>\u2190 Run this cell to save your annotations.</file>\n",
    "agile.save_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd07UJ1q2c4x"
   },
   "source": [
    "## 7. Improve your classifier further using these search results\n",
    "\n",
    "You can now go back to the _Build a machine learning classifier model_ section to retrain the classifier based on all annotations provided so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iqGtCPknQFR"
   },
   "source": [
    "# Saving your classifier and running inference\n",
    "\n",
    "The trained classifier consists of the following elements\n",
    "1. The *weights* of the model (how to multiply and add the embedding values together to produce higher scores for the examples of the target class than other examples), which were learned during training.\n",
    "2. The model *bias* (how to shift the scores so that scores for positive examples are positive and vice-versa), also learned during training.\n",
    "3. The list of labels (class names) corresponding to the output scores\n",
    "4. Some metadata related to the model that created the embeddings, so that if the classifier is used on new audio, we make sure to embed in a compatible way.\n",
    "\n",
    "With this information, the classifier can be saved and used on other search sets later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHFaxBbhnQFR"
   },
   "outputs": [],
   "source": [
    "classifier_name = 'ybg_01'  #@param {type:'string'}\n",
    "\n",
    "classifier_path = config.models_folder / f'{classifier_name}.json'\n",
    "\n",
    "classifier_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "agile.classifier.save(classifier_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2krifHJZnQFR"
   },
   "source": [
    "We can also run the model over all of the search dataset and save the results to a csv. Specify:\n",
    "1. The csv filename to save the results to\n",
    "2. The threshold. Anything above the threshold for the target labels will be saved. Typically this would be zero to save anything the classifier believes is the target class\n",
    "3. Which labels to include. Leave it as None to include all the labels you trained for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJMfJJ2nnQFR"
   },
   "outputs": [],
   "source": [
    "output_filename = 'ybg_output.csv'  #@param {type:'string'}\n",
    "\n",
    "threshold=0.0 #@param {type:'string'}\n",
    "\n",
    "# You can also specify a random subset of the dataset to run inference on, e.g. 0.5 for 50%\n",
    "subset = 0.1 #@param {type:'string'}\n",
    "\n",
    "# Which labels to include in the output file. If None, all labels are included.\n",
    "labels = None\n",
    "\n",
    "output_filepath = config.predictions_folder / output_filename\n",
    "output_filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "agile.run_inference(output_filepath, threshold=0.0, dataset=config.search_dataset_name, subset=subset)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "QF6o2WW91NlC"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
